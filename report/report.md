# 实验报告

## 小组分工

陈楷予：使用LightGBM模型进行实验；通过调换两队特征顺序并修改label扩充训练集数据；对原始特征进行扩充和处理（second version）。 

郑逸宁：使用多个模型进行集成训练(final version)；代码整合及对运行需求等进行整理。 

李艾丽丝：使用XGBoost和原始特征数据构建baseline模型（first version）；加入在数据集上验证有效的英雄属性特征(third version)。

## Kaggle比赛说明 成绩截图

在比赛最终70%的数据集的排行榜上，我们最好的模型排名第三。

![privateLeaderboard](./privateLeaderboard.png)

在比赛过程中30%的数据集的排行榜上，我们最好的模型排名第四。

![publicLeaderboard](./publicLeaderboard.png)

## 代码和目录结构，运行文档

代码分为三个部分，均放置在 code 文件夹内。

+ work.py 入口程序，配置了运行选项

+ dataloader.py 包含数据读取和特征提取的代码

+ hero_feats.py 包含对英雄特征的提取代码

在 code 文件夹下运行 `pip install -r requirements` 安装依赖包；
运行 `python work.py` 加参数运行代码。

```shell
# 运行 First Version 的基线程序
python work.py -b

# 运行 Second Version 的程序
python work.py -s

# 运行 Third Version 和 Forth Version 的程序（N 为集成模型的数量，不写默认为1）
python work.py -a (N)
```

## 问题描述 数据描述 数据可视化

cky

## 算法

### 决策树 XGBoost LightGBM

1. 决策树： 
   
   决策树是一种树状结构，树中每个非叶子节点代表一种属性的测试，每个分支表示在该属性上的某个值域划分，树的叶子节点给出结果的类标。在决策时，从根节点出发对数据属性依次检查匹配，最终根据到达的叶节点给出预测结果。决策树通常可分为分类树和回归树两种，两种树的目标值类型不同，在构建决策树时，两种树的最大不同在于如何分裂节点。对于分类树，目标通常是一组离散的值（如ID3.5、C4.5等算法），在构造决策树时，算法每一轮计算对每个属性值计算划分后的条件熵，并与原始划分进行比较（如计算信息增益、信息增益率、gini系数等），每一轮选择一个最具区分度的属性并为其构建节点，直到叶子节点。对于回归树，目标通常是连续值（如CART等算法），在构造决策树时，算法每一轮不再计算熵值，而是对每个属性的不同划分，取划分区域的目标均值作为分裂节点的预测值，计算预测值和真实值的均方误差，通过最小化均方误差选择最优的划分属性和划分点并为其构建节点和分支，直到叶子节点。
2. XGBoost：
   
   XGBoost是基于GBDT（Gradient Boosting Decision Tree）算法的改进，支持并行操作，具有更高的速度和效率，其核心原理还是GBDT算法，在下面的内容中，我们将简要介绍GBDT算法的核心思路。 

   GBDT算法是使用梯度提升的提升树算法，提升树算法（Boosting Decision Tree）通过迭代学习，每轮迭代时，使用之前所有回归树的结论和残差拟合成一个新的残差回归树，迭代数轮生成若干回归树，每一颗回归树都在原来的学习结果基础上继续学习，以此集成。GBDT算法则是使用损失函数的负梯度来拟合残差，其算法流程和思路与提升树算法相同。

   由于XGBoost（GBDT）的内核是回归树，当使用XGBoost（或GBDT）解决k分类问题时，对算法有一定的调整，算法在每轮迭代时不是建一颗回归树，而是创建k颗回归树，对k颗回归树的预测值使用softmax归一成概率分布，取概率最高的树编号作为分类结果。
3. LightGBM：
   
   LightGBM其本质也是一种梯度boosting算法，通过迭代生成若干回归树进行集成学习，在生成回归树时使用负梯度拟合残差，其核心原理与GBDT类似。相比XGBoost，LightGBM具有轻量、低内存占用、更快的训练效率、能够处理更大规模的数据几大优势。在下面的内容中，我们将简要介绍LightGBM相比XGBoost作出的一些主要改动。

   在XGBoost算法中，每轮迭代均需多次遍历全部的训练数据，若将所有训练数据加载进内存，则占用了大量内存空间且导致可处理的数据规模受限。在LightGBM算法中，使用了一种基于Histogram的决策树算法，其基本思路为，将连续的属性值分布离散化成k个整数，构造一个宽度为k的直方图，在遍历数据时，一次遍历即使直方图累积了所有需要的统计量，通过此方法降低了对训练数据的存取需求，降低LightGBM的内存占用，使其能够处理更大规模的数据。

   在此前的决策树算法中，构建树结构通常采用一种level-wise的方法，即每次分裂一层节点，由于很多节点的分裂增益较低，没必要进行搜索和分裂，因此该方法带来了很多没必要的开销；在LightGBM算法中，构建树结构时采用了一种leaf-wise的方法，即每次从所有节点中，选择分裂增益最大的节点进行分裂，同时限制分裂深度，通过此方法降低了算法开销，获得了更高的精度。

### 特征提取

1. 提供的数据 
   
   比赛主办方共提供了两份数据，一份是两方队伍在游戏前5分钟的游戏情况，包括两队队员使用的英雄角色编号、队员的经验值、两队前5分钟的购买情况、一杀情况等等特征；另一份数据（hero_names.json）详细罗列了英雄角色的属性，包括英雄的攻击类型、角色属性、基础健康值、攻击力、移动速度等等。

   我们使用了第一份数据（两队的游戏前5分钟数据）的所有特征（共 ？ 个），同时对两队队员使用的不同英雄角色，加入了从第二份数据中提取出的属性特征，具体分析过程和最终使用的特征将在下一节中介绍。
2. 英雄属性特征 
   
   2.1 预分析 

   我们对第二份数据文件中每个英雄的其中20个不同属性信息进行了分析，对属性值类型为字符串的，我们统计了该属性值对不同英雄的不同取值的个数：
   属性名 | primary_attr | attack_type | roles
   :-: | :-: | :-: | :-:
   不同取值个数 | 3 | 2 | 9
   对属性值类型为数值类型的，我们统计了其对不同英雄可能取到的最大值、最小值和平均值，部分属性的统计结果如下：
   属性名 | base_health | base_health_regen | base_mana | base_mana_regen | base_armor | ...
   :-: | :-: | :-: | :-: | :-: | :-: | :-:
   max | 200 | 3.25 | 75 | 0 | 7 | ...
   min | 200 | -9999 | 75 | 0 | -3 | ...
   mean | 200.0 | -7263.96 | 75.0 | 0.0 | 0.047 | ...
   对base_health、base_mana、base_mana_regen三个属性而言，不同的英雄取值相同，没有区分度，因此不应作为特征纳入，对base_health_regen，由于一些英雄在该属性上的取值为空，因此我们也舍弃了这个属性。

   2.2 特征处理和选择 

   在实验时，我们将英雄的roles属性编码成one-hot向量，其他的属性仍使用原来的值（对另外两个属性值类型为字符串的属性，我们对每个值赋一个整型数值作为编码），对每一个英雄，其属性特征向量包含16个属性，维度为24。

   我们经过实验发现，使用全部的英雄属性特征对结果并没有提升反而会使结果下降，我们最终保留了英雄属性中的roles特征，最终提交的结果在测试集上roc为0.723，排名第六。

### Ensemble

zyn

### tricks

cky

## 运行结果和可视化(决策树可视化)

cky 或 zyn
**cky**

## 算法

### 草稿

#### First Version -- 0.614

我们将所有的columns作为features，作为一个二分类任务送给XGBoost进行分类。

#### Second Version -- 0.721

(+4%) 我们将输出的submission的二分类0/1输出，变为概率输出。

(+6%) 为了让英雄的使用情况体现在Feature中，我们将双方的10个英雄展开到 10 * 112 (英雄个数) * 7 (英雄feature) 维度上。

(+1%) 我们把交换双方后的数据也加入训练集中，使得训练数据翻倍。

#### Third Version -- 0.723

(+0.2%) 对比赛中的每个角色使用的英雄添加onehot的roles特征

#### Forth Version -- 0.725

(+0.2%) 使用3-9个相同的第三版的模型进行集成学习，结果逐渐接近0.725。 


### 模型集成

我们使用3-9个第三版的模型进行集成学习，在公开数据集上使结果逐渐接近0.725。我们编写了可以控制集成模型的个数的启动参数，可以方便地进行多次实验。

### tricks

**cky**

## 运行结果和可视化(决策树可视化)

**cky**



第三版的模型运行结果如下图:

![runtime](./runtime.png)

特征重要性可视化如下图:

![featureImportance](./featureImportance.png)

